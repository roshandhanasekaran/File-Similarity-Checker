# -*- coding: utf-8 -*-
"""similarity.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aF8xJZ-kHvjTl8hljrfkF1FpkYtTIocH
"""

pip install PyMuPDF

pip install transformers

import os
import fitz  # PyMuPDF
import docx
import nltk
import string
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# Download NLTK data
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

# Text pre-processing function
def preprocess_text(text):
    # Lowercase
    text = text.lower()

    # Remove punctuation
    text = text.translate(str.maketrans('', '', string.punctuation))

    # Tokenize
    words = nltk.word_tokenize(text)

    # Remove stopwords
    words = [word for word in words if word not in nltk.corpus.stopwords.words('english')]

    # Lemmatize
    lemmatizer = nltk.WordNetLemmatizer()
    words = [lemmatizer.lemmatize(word) for word in words]

    return words

# Read docx text
def read_docx(file_path):
    doc = docx.Document(file_path)
    full_text = []
    for para in doc.paragraphs:
        full_text.append(para.text)
    return ' '.join(full_text)

# Read pdf text
def read_pdf(file_path):
    with fitz.open(file_path) as doc:
        text = ''
        for page_num in range(doc.page_count):
            text += doc.load_page(page_num).get_text('text')
    return text

# Get file paths
file1_path = input("Enter the path of the first file (docx/pdf): ")
file2_path = input("Enter the path of the second file (docx/pdf): ")

# Read files
if file1_path.lower().endswith('.docx'):
    text1 = read_docx(file1_path)
elif file1_path.lower().endswith('.pdf'):
    text1 = read_pdf(file1_path)
else:
    raise ValueError("Unsupported file format for file 1. Only .docx and .pdf are supported.")

if file2_path.lower().endswith('.docx'):
    text2 = read_docx(file2_path)
elif file2_path.lower().endswith('.pdf'):
    text2 = read_pdf(file2_path)
else:
    raise ValueError("Unsupported file format for file 2. Only .docx and .pdf are supported.")

# Pre-process texts
processed_text1 = preprocess_text(text1)
processed_text2 = preprocess_text(text2)

# Find common words
common_words = set(processed_text1).intersection(set(processed_text2))

# Count repeated words in both files
repeated_words = {word: processed_text1.count(word) + processed_text2.count(word) for word in common_words}

# Sort repeated words by count in descending order
sorted_repeated_words = dict(sorted(repeated_words.items(), key=lambda x: x[1], reverse=True))

# Highlight similarities in the original texts
def highlight_similarities(text, common_words):
    highlighted_text = ""
    for word in nltk.word_tokenize(text):
        if word.lower() in common_words:
            highlighted_text += "\033[1;31m" + word + "\033[0m "
        else:
            highlighted_text += word + " "
    return highlighted_text.strip()

highlighted_text1 = highlight_similarities(text1, common_words)
highlighted_text2 = highlight_similarities(text2, common_words)

print("\nHighlighted similarities in text 1:")
print(highlighted_text1)

print("\nHighlighted similarities in text 2:")
print(highlighted_text2)

# Sort repeated words by count in descending order
sorted_repeated_words = dict(sorted(repeated_words.items(), key=lambda x: x[1], reverse=True))

# Compute tf-idf vectors
vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform([' '.join(processed_text1), ' '.join(processed_text2)])

# Compute cosine similarity
similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])

# Print similarity percentage
print("\nSimilarity percentage:", similarity[0][0] * 100)


print("\nRepeated words count in both files:")
for word, count in sorted_repeated_words.items():
    print(word, ":", count)